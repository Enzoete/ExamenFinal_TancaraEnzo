{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeqlwlNHLJlVyvO+jkiPdF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Enzoete/ExamenFinal_TancaraEnzo/blob/main/Documento_Examen_Final_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generaci√≥n y Transferencia de Poses Humanas en Blender Mediante Redes Neuronales Preentrenadas\n",
        "## Enzo Tancara Mamani ‚Äì COM300 ‚Äì Examen Final\n"
      ],
      "metadata": {
        "id": "x-S81OvrIddM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objetivo\n",
        "\n",
        "El objetivo de este proyecto fue desarrollar un procedimiento multimedial que integre inteligencia artificial con Blender, utilizando redes neuronales preentrenadas para detectar poses humanas desde im√°genes 2D y transferir dicha informaci√≥n a un personaje tridimensional en Blender mediante scripting en Python.\n"
      ],
      "metadata": {
        "id": "V9UeHXR5IkOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flujo de trabajo del proyecto\n",
        "\n",
        "1. Se utiliz√≥ **Google Colab** para aplicar MediaPipe Pose (una red neuronal preentrenada) sobre im√°genes de entrada.\n",
        "2. Se extrajeron los puntos clave (landmarks) del cuerpo humano en 2D.\n",
        "3. Los puntos fueron exportados en un archivo `.json`.\n",
        "4. En Blender, se construy√≥ un personaje con rig y se importaron los datos del `.json`.\n",
        "5. A trav√©s de scripts en Python, se aplicaron las poses detectadas al rig del personaje"
      ],
      "metadata": {
        "id": "6uJbQA_pI1LT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Red neuronal utilizada\n",
        "\n",
        "Se emple√≥ **MediaPipe Pose**, un modelo preentrenado desarrollado por Google, basado en redes neuronales convolucionales (CNN). Este modelo permite detectar hasta 33 puntos claves del cuerpo humano, incluyendo caderas, hombros, extremidades y cabeza, tanto en 2D como en 3D.\n",
        "\n",
        "MediaPipe fue ejecutado desde Colab por ser un entorno optimizado para esa librer√≠a y por facilitar la visualizaci√≥n interactiva de los resultados.\n"
      ],
      "metadata": {
        "id": "aA8v4t6sIyZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¬øC√≥mo funciona y c√≥mo fue entrenada MediaPipe Pose?\n",
        "MediaPipe Pose es un modelo de inteligencia artificial desarrollado por Google para detectar autom√°ticamente poses humanas en im√°genes o video. Est√° basado en una arquitectura de redes neuronales convolucionales (CNNs), espec√≠ficamente adaptadas para detecci√≥n de poses, y fue preentrenado con grandes conjuntos de datos de im√°genes de personas realizando distintas actividades f√≠sicas.\n",
        "\n",
        "### Proceso de entrenamiento\n",
        "MediaPipe Pose fue entrenado con datasets de visi√≥n por computadora que contienen im√°genes etiquetadas con las posiciones exactas de las articulaciones del cuerpo humano (por ejemplo: COCO dataset, BlazePose dataset).\n",
        "\n",
        "La red neuronal aprende a detectar 33 puntos clave del cuerpo: cabeza, cuello, hombros, codos, mu√±ecas, caderas, rodillas, tobillos, entre otros.\n",
        "\n",
        "Utiliza dos etapas:\n",
        "\n",
        "Detecci√≥n del cuerpo completo en la imagen (con una red \"detector\").\n",
        "\n",
        "Estimaci√≥n precisa de los puntos clave usando una red refinadora (\"pose tracker\").\n",
        "\n",
        "Repositorio principal: https://github.com/google-ai-edge/mediapipe\n",
        "\n"
      ],
      "metadata": {
        "id": "yGZuyrZGSI5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicaci√≥n de la inteligencia artificial (modelo de pose humana)\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose(static_image_mode=True)\n",
        "    results = pose.process(image_rgb)\n",
        "Estas tres l√≠neas son el coraz√≥n del modelo de IA:\n",
        "\n",
        "mp_pose.Pose(...) carga el modelo MediaPipe Pose en modo est√°tico.\n",
        "\n",
        ".process(image_rgb) aplica la red neuronal a la imagen cargada.\n",
        "\n",
        "El resultado contiene las coordenadas 2D normalizadas (entre 0 y 1) de 33 puntos clave del cuerpo humano.\n",
        "\n",
        "En este paso, la red neuronal ya hizo su trabajo: analizar la imagen y estimar la posici√≥n de las articulaciones.\n",
        "\n",
        "lm = results.pose_landmarks.landmark\n",
        "Se calculan coordenadas medias (cuello y cadera) y se extraen puntos espec√≠ficos relevantes para rigging\n",
        "\n",
        "\n",
        "Aqu√≠ se convierten los resultados del modelo en coordenadas comprensibles. Se agrupan ciertos puntos para generar referencias centrales del cuerpo (como el cuello) y se filtran los puntos clave que se usar√°n para construir el esqueleto (stickman).\n",
        "\n"
      ],
      "metadata": {
        "id": "rD1Pqq7wUVng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üì• Subir imagen\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]\n",
        "\n",
        "# üîß Librer√≠as\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import json\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# üñºÔ∏è Cargar imagen\n",
        "image = cv2.imread(image_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "h, w, _ = image.shape\n",
        "\n",
        "# Mostrar imagen original\n",
        "display(Image.open(image_path))\n",
        "\n",
        "# üï∫ Detecci√≥n de pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=True)\n",
        "results = pose.process(image_rgb)\n",
        "\n",
        "# üé® Visualizaci√≥n y extracci√≥n\n",
        "image_stick = image_rgb.copy()\n",
        "keypoints_2d = {}\n",
        "\n",
        "if results.pose_landmarks:\n",
        "    lm = results.pose_landmarks.landmark\n",
        "\n",
        "    # Coordenadas hombros y caderas\n",
        "    l_shoulder, r_shoulder = lm[11], lm[12]\n",
        "    l_hip, r_hip = lm[23], lm[24]\n",
        "\n",
        "    # Punto medio del cuello\n",
        "    neck_x = (l_shoulder.x + r_shoulder.x) / 2\n",
        "    neck_y = (l_shoulder.y + r_shoulder.y) / 2\n",
        "    keypoints_2d[\"landmark_100\"] = {\"x\": neck_x, \"y\": neck_y}\n",
        "\n",
        "    # Punto medio de la cadera\n",
        "    hip_x = (l_hip.x + r_hip.x) / 2\n",
        "    hip_y = (l_hip.y + r_hip.y) / 2\n",
        "    keypoints_2d[\"landmark_101\"] = {\"x\": hip_x, \"y\": hip_y}\n",
        "\n",
        "    # Guardar puntos reales necesarios\n",
        "    indices = [0, 11, 12, 13, 14, 15, 16, 21, 22,\n",
        "               23, 24, 25, 26, 27, 28, 31, 32]\n",
        "    for idx in indices:\n",
        "        keypoints_2d[f\"landmark_{idx}\"] = {\"x\": lm[idx].x, \"y\": lm[idx].y}\n",
        "\n",
        "    # Conexiones stickman\n",
        "    connections = [\n",
        "        (0, 100),         # Cabeza a cuello\n",
        "        (100, 11), (100, 12),  # Cuello a hombros\n",
        "        (100, 101),       # Cuello a cadera central\n",
        "        (101, 23), (101, 24),  # Cadera central a caderas\n",
        "\n",
        "        (11, 13), (13, 15), (15, 21),  # Brazo izq\n",
        "        (12, 14), (14, 16), (16, 22),  # Brazo der\n",
        "\n",
        "        (23, 25), (25, 27), (27, 31),  # Pierna izq\n",
        "        (24, 26), (26, 28), (28, 32)   # Pierna der\n",
        "    ]\n",
        "\n",
        "    # Dibujar l√≠neas\n",
        "    for start, end in connections:\n",
        "        def get_coords(idx):\n",
        "            if idx == 100:\n",
        "                return int(neck_x * w), int(neck_y * h)\n",
        "            elif idx == 101:\n",
        "                return int(hip_x * w), int(hip_y * h)\n",
        "            else:\n",
        "                return int(lm[idx].x * w), int(lm[idx].y * h)\n",
        "\n",
        "        p1 = get_coords(start)\n",
        "        p2 = get_coords(end)\n",
        "        cv2.line(image_stick, p1, p2, (0, 255, 0), 3)\n",
        "\n",
        "    # Articulaciones visibles (incluye hombros y caderas)\n",
        "    articulaciones = [11, 12, 13, 14, 15, 16, 21, 22,\n",
        "                      23, 24, 25, 26, 27, 28, 31, 32]\n",
        "    for idx in articulaciones:\n",
        "        cx, cy = int(lm[idx].x * w), int(lm[idx].y * h)\n",
        "        cv2.circle(image_stick, (cx, cy), 5, (255, 0, 0), -1)\n",
        "\n",
        "    # Cuello y cadera central en rojo\n",
        "    cv2.circle(image_stick, (int(neck_x * w), int(neck_y * h)), 5, (0, 0, 255), -1)\n",
        "    cv2.circle(image_stick, (int(hip_x * w), int(hip_y * h)), 5, (0, 0, 255), -1)\n",
        "\n",
        "# üñºÔ∏è Mostrar resultado final\n",
        "display(Image.fromarray(image_stick))\n",
        "\n",
        "# üíæ Guardar JSON\n",
        "with open(\"pose_data_2D.json\", \"w\") as f:\n",
        "    json.dump(keypoints_2d, f, indent=2)\n",
        "\n",
        "# üì§ Descargar JSON\n",
        "files.download(\"pose_data_2D.json\")"
      ],
      "metadata": {
        "id": "a0MFFapzR05T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este c√≥digo demuestra c√≥mo se puede integrar un modelo de IA preentrenado en un flujo de trabajo multimedial. La red neuronal realiza la detecci√≥n sin necesidad de entrenamiento adicional, y los resultados se procesan para construir una visualizaci√≥n comprensible, as√≠ como un archivo estructurado que puede ser consumido por otros entornos como Blender."
      ],
      "metadata": {
        "id": "obmm3CmpXLpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Integraci√≥n en Blender\n",
        "\n",
        "Se utiliz√≥ el archivo `.json` generado en Colab para extraer las coordenadas de las articulaciones clave. Desde Blender se construy√≥ un rig con los mismos nombres de huesos, y se escribieron scripts en Python que leen el archivo, calculan vectores de direcci√≥n y aplican la rotaci√≥n correspondiente a cada hueso del personaje.\n",
        "\n",
        "Debido a limitaciones t√©cnicas, el modelo IA no pudo ser ejecutado directamente dentro de Blender, por lo que se recurri√≥ a una integraci√≥n externa desde Colab.\n"
      ],
      "metadata": {
        "id": "Zukb8ZZhJAzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 Aplicaci√≥n en Blender con Python: visualizaci√≥n de puntos clave\n",
        "Una vez obtenido el archivo pose_data_2D.json generado en Google Colab por la red neuronal preentrenada MediaPipe Pose, el siguiente script fue utilizado en Blender para leer ese archivo y visualizar los puntos clave (landmarks) como objetos Empty.\n",
        "\n",
        "üîß Objetivo\n",
        "Este script:\n",
        "\n",
        "Abre el archivo .json con las coordenadas estimadas por la IA.\n",
        "\n",
        "Convierte cada punto en un objeto visual en Blender (tipo Empty en forma de esfera).\n",
        "\n",
        "Escala las posiciones para que puedan visualizarse correctamente dentro del espacio 3D.\n",
        "\n",
        "Opcionalmente borra cualquier marcador previo creado por ejecuciones anteriores.\n",
        "\n",
        "MediaPipe entrega coordenadas normalizadas entre 0 y 1, donde (0,0) est√° en la esquina superior izquierda de la imagen.\n",
        "\n",
        "Estas coordenadas se adaptan al sistema de Blender, donde se trabaja en una escala 3D centrada.\n",
        "\n",
        "El eje vertical de la imagen (Y) se convierte al eje Z de Blender, por eso se invierte con (0.5 - y)."
      ],
      "metadata": {
        "id": "111QYbBwJS1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bpy\n",
        "import json\n",
        "\n",
        "# ‚úÖ Ruta a tu archivo JSON\n",
        "json_path = \"C:/Users/Enzo/Desktop/IA_Final/2/pose_data_2D3.json\"\n",
        "\n",
        "# üîÑ Escala para hacer la pose visible (ajustable)\n",
        "scale = 10\n",
        "\n",
        "# üìÅ Leer JSON\n",
        "with open(json_path) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# üßπ Eliminar empties anteriores (opcional)\n",
        "for obj in bpy.data.objects:\n",
        "    if obj.name.startswith(\"LM_\"):\n",
        "        bpy.data.objects.remove(obj)\n",
        "\n",
        "# üß† Crear un empty para cada punto del JSON\n",
        "for lm_name, coords in data.items():\n",
        "    x = (coords[\"x\"] - 0.5) * scale\n",
        "    z = (0.5 - coords[\"y\"]) * scale\n",
        "    pos = (x, 0, z)\n",
        "\n",
        "    empty = bpy.data.objects.new(f\"LM_{lm_name}\", None)\n",
        "    empty.location = pos\n",
        "    empty.empty_display_size = 0.05\n",
        "    empty.empty_display_type = 'SPHERE'\n",
        "    bpy.context.collection.objects.link(empty)\n",
        "\n",
        "print(\"‚úÖ Puntos creados\")"
      ],
      "metadata": {
        "id": "bhjMMLfSYBo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Generaci√≥n del stickman en Blender a partir de los puntos IA\n",
        "Este script lee el archivo JSON generado por MediaPipe Pose y reconstruye una figura humana simplificada en 3D, conectando puntos clave mediante l√≠neas (segmentos). As√≠ se genera un ‚Äústickman‚Äù que sirve como gu√≠a visual y base para animaci√≥n o rigging.\n",
        "\n",
        "üß© ¬øQu√© hace el script?\n",
        "Define las conexiones entre articulaciones (por ejemplo: hombro ‚Üí codo ‚Üí mu√±eca).\n",
        "\n",
        "Lee las coordenadas desde pose_data_2D.json (previamente generado).\n",
        "\n",
        "Convierte las coordenadas normalizadas a posiciones 3D escaladas.\n",
        "\n",
        "Crea l√≠neas 3D (segmentos) usando from_pydata entre cada par de puntos.\n",
        "\n",
        "Muestra todo como objeto Wireframe, visible en modo frontal para mayor claridad.\n",
        "\n",
        "Los puntos centrales extra como cuello (100) y cadera central (101) fueron a√±adidos manualmente en la detecci√≥n para permitir una mejor jerarqu√≠a anat√≥mica.\n",
        "\n",
        "Se usa el eje Z como altura, ya que los valores y de imagen se invierten para coincidir con el espacio de Blender.\n",
        "\n",
        "Los objetos antiguos que empiezan con \"Segment_\" se eliminan autom√°ticamente para evitar duplicados."
      ],
      "metadata": {
        "id": "WIp-eFdXY9Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bpy\n",
        "import json\n",
        "import mathutils\n",
        "\n",
        "# ‚úÖ Ruta al archivo JSON (actualiz√° con tu propia ruta si cambia)\n",
        "json_path = \"C:/Users/Enzo/Desktop/IA_Final/2/pose_data_2D3.json\"\n",
        "scale = 10  # Escala de visualizaci√≥n\n",
        "\n",
        "# üß± Conexiones del stickman, incluyendo cuello (100) y cadera central (101)\n",
        "connections = [\n",
        "    (0, 100),         # Cabeza a cuello\n",
        "    (100, 11), (100, 12),  # Cuello a hombros\n",
        "    (100, 101),            # Cuello a cadera central\n",
        "    (101, 23), (101, 24),  # Cadera central a caderas\n",
        "\n",
        "    (11, 13), (13, 15), (15, 21),  # Brazo izquierdo completo\n",
        "    (12, 14), (14, 16), (16, 22),  # Brazo derecho completo\n",
        "\n",
        "    (23, 25), (25, 27), (27, 31),  # Pierna izquierda\n",
        "    (24, 26), (26, 28), (28, 32)   # Pierna derecha\n",
        "]\n",
        "\n",
        "# üìÇ Leer archivo JSON\n",
        "with open(json_path) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# üìå Funci√≥n para obtener posici√≥n en 3D\n",
        "def get_pos(idx):\n",
        "    key = f\"landmark_{idx}\"\n",
        "    if key not in data:\n",
        "        raise ValueError(f\"‚ùó PUNTO FALTANTE: {key}\")\n",
        "    x = (data[key][\"x\"] - 0.5) * scale\n",
        "    z = (0.5 - data[key][\"y\"]) * scale\n",
        "    return mathutils.Vector((x, 0, z))\n",
        "\n",
        "# üßπ Limpiar objetos anteriores del tipo \"Segment_\"\n",
        "for obj in bpy.data.objects:\n",
        "    if obj.name.startswith(\"Segment_\"):\n",
        "        bpy.data.objects.remove(obj, do_unlink=True)\n",
        "\n",
        "# ü™¢ Crear segmentos como l√≠neas entre puntos\n",
        "for i, (a, b) in enumerate(connections):\n",
        "    try:\n",
        "        p1 = get_pos(a)\n",
        "        p2 = get_pos(b)\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "        continue\n",
        "\n",
        "    mesh = bpy.data.meshes.new(f\"SegmentMesh_{i}\")\n",
        "    obj = bpy.data.objects.new(f\"Segment_{i}\", mesh)\n",
        "    bpy.context.collection.objects.link(obj)\n",
        "    mesh.from_pydata([p1, p2], [(0, 1)], [])\n",
        "    mesh.update()\n",
        "\n",
        "    obj.show_in_front = True\n",
        "    obj.display_type = 'WIRE'\n",
        "\n",
        "print(\"‚úÖ Stickman 3D generado con cuello y cadera incluidos\")"
      ],
      "metadata": {
        "id": "vSHwMksyY9Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creaci√≥n autom√°tica de rig en Blender desde datos IA\n",
        "A partir de los keypoints obtenidos con MediaPipe Pose, este script construye un esqueleto completo y articulado en Blender. La estructura resultante es un rig funcional que representa fielmente la postura estimada en 2D, proyectada en un espacio tridimensional.\n",
        "\n",
        "‚öôÔ∏è Funciones del script\n",
        "Lee un archivo .json con las coordenadas generadas por la IA.\n",
        "\n",
        "Calcula un punto intermedio virtual (landmark_102) entre el cuello y la cadera para dividir el torso en dos partes.\n",
        "\n",
        "Define la estructura √≥sea en t√©rminos de nombres, jerarqu√≠a y posiciones absolutas.\n",
        "\n",
        "Crea autom√°ticamente un Armature en modo Edit y posiciona cada hueso seg√∫n sus puntos de origen y destino en el plano frontal (X-Z).\n",
        "\n",
        "Asigna relaciones de padre‚Äìhijo a cada secci√≥n del cuerpo, lo que permite animaciones completas."
      ],
      "metadata": {
        "id": "wzOsWsCXZdgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bpy\n",
        "import json\n",
        "import mathutils\n",
        "\n",
        "# üìÇ Ruta al archivo JSON\n",
        "json_path = \"C:/Users/Enzo/Desktop/IA_Final/2/pose_data_2D3.json\"\n",
        "scale = 10\n",
        "arm_name = \"StickmanRig\"\n",
        "\n",
        "# üìñ Leer JSON\n",
        "with open(json_path) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# üß† Calcular landmark virtual 102 (punto medio entre cuello 100 y cadera 101)\n",
        "def calc_midpoint(id1, id2, new_id):\n",
        "    p1 = data[f\"landmark_{id1}\"]\n",
        "    p2 = data[f\"landmark_{id2}\"]\n",
        "    mx = (p1[\"x\"] + p2[\"x\"]) / 2\n",
        "    my = (p1[\"y\"] + p2[\"y\"]) / 2\n",
        "    data[f\"landmark_{new_id}\"] = {\"x\": mx, \"y\": my}\n",
        "\n",
        "calc_midpoint(100, 101, 102)\n",
        "\n",
        "# üß± Definici√≥n de huesos\n",
        "bone_connections = [\n",
        "    (\"head\", 0, 100),\n",
        "    (\"neck\", 100, 102),\n",
        "    (\"spine_lower\", 102, 101),\n",
        "\n",
        "    (\"shoulder_L\", 100, 11), (\"upper_arm_L\", 11, 13), (\"forearm_L\", 13, 15), (\"hand_L\", 15, 21),\n",
        "    (\"shoulder_R\", 100, 12), (\"upper_arm_R\", 12, 14), (\"forearm_R\", 14, 16), (\"hand_R\", 16, 22),\n",
        "\n",
        "    (\"hip_L\", 101, 23), (\"thigh_L\", 23, 25), (\"shin_L\", 25, 27), (\"foot_L\", 27, 31),\n",
        "    (\"hip_R\", 101, 24), (\"thigh_R\", 24, 26), (\"shin_R\", 26, 28), (\"foot_R\", 28, 32)\n",
        "]\n",
        "\n",
        "# üîç Convertir puntos a coordenadas 3D\n",
        "def get_point(idx):\n",
        "    key = f\"landmark_{idx}\"\n",
        "    if key not in data:\n",
        "        raise ValueError(f\"‚ö†Ô∏è Faltante: {key}\")\n",
        "    x = (data[key][\"x\"] - 0.5) * scale\n",
        "    z = (0.5 - data[key][\"y\"]) * scale\n",
        "    return mathutils.Vector((x, 0, z))\n",
        "\n",
        "# üßπ Limpiar armature previa\n",
        "for obj in bpy.data.objects:\n",
        "    if obj.type == 'ARMATURE' and obj.name == arm_name:\n",
        "        bpy.data.objects.remove(obj, do_unlink=True)\n",
        "\n",
        "# ü¶¥ Crear nuevo rig\n",
        "bpy.ops.object.add(type='ARMATURE', enter_editmode=True)import bpy\n",
        "import mathutils\n",
        "\n",
        "# ‚öôÔ∏è Nombres de los rigs (ajustalos seg√∫n tu escena)\n",
        "source_rig = bpy.data.objects[\"StickmanRig\"]\n",
        "target_rig = bpy.data.objects[\"StickmanRig22\"]\n",
        "\n",
        "# Leer direcci√≥n √≥sea de StickmanRig en Edit Mode (pose en reposo)\n",
        "bpy.context.view_layer.objects.active = source_rig\n",
        "bpy.ops.object.mode_set(mode='EDIT')\n",
        "source_dirs = {}\n",
        "for b in source_rig.data.edit_bones:\n",
        "    vec = b.tail - b.head\n",
        "    if vec.length > 0:\n",
        "        source_dirs[b.name] = vec.normalized()\n",
        "\n",
        "# Leer direcci√≥n √≥sea de StickmanRig22 en Edit Mode (para comparar espacio)\n",
        "bpy.context.view_layer.objects.active = target_rig\n",
        "bpy.ops.object.mode_set(mode='EDIT')\n",
        "target_dirs = {}\n",
        "for b in target_rig.data.edit_bones:\n",
        "    vec = b.tail - b.head\n",
        "    if vec.length > 0:\n",
        "        target_dirs[b.name] = vec.normalized()\n",
        "\n",
        "# Volver a Pose Mode para aplicar rotaciones\n",
        "bpy.ops.object.mode_set(mode='POSE')\n",
        "\n",
        "for bone in target_rig.pose.bones:\n",
        "    name = bone.name\n",
        "    if name in source_dirs and name in target_dirs:\n",
        "        from_dir = target_dirs[name]\n",
        "        to_dir = source_dirs[name]\n",
        "\n",
        "        if from_dir.length > 0 and to_dir.length > 0:\n",
        "            rot = from_dir.rotation_difference(to_dir)\n",
        "            bone.rotation_mode = 'QUATERNION'\n",
        "            bone.rotation_quaternion = rot\n",
        "\n",
        "# Aplicar la pose visualmente\n",
        "bpy.ops.pose.select_all(action='SELECT')\n",
        "bpy.ops.pose.visual_transform_apply()\n",
        "\n",
        "print(\"‚úÖ La Rest Pose del StickmanRig fue aplicada como pose activa en StickmanRig22.\")\n",
        "\n",
        "arm = bpy.context.object\n",
        "arm.name = arm_name\n",
        "arm.show_in_front = True\n",
        "edit_bones = arm.data.edit_bones\n",
        "created_bones = {}\n",
        "\n",
        "# üîß Crear huesos\n",
        "for name, start_idx, end_idx in bone_connections:\n",
        "    bone = edit_bones.new(name)\n",
        "    bone.head = get_point(start_idx)\n",
        "    bone.tail = get_point(end_idx)\n",
        "    bone.roll = 0\n",
        "    created_bones[name] = bone\n",
        "\n",
        "# üß∑ Definir jerarqu√≠a\n",
        "created_bones[\"neck\"].parent = created_bones[\"head\"]\n",
        "created_bones[\"spine_lower\"].parent = created_bones[\"neck\"]\n",
        "\n",
        "# Brazo izquierdo\n",
        "created_bones[\"shoulder_L\"].parent = created_bones[\"neck\"]\n",
        "created_bones[\"upper_arm_L\"].parent = created_bones[\"shoulder_L\"]\n",
        "created_bones[\"forearm_L\"].parent = created_bones[\"upper_arm_L\"]\n",
        "created_bones[\"hand_L\"].parent = created_bones[\"forearm_L\"]\n",
        "\n",
        "# Brazo derecho\n",
        "created_bones[\"shoulder_R\"].parent = created_bones[\"neck\"]\n",
        "created_bones[\"upper_arm_R\"].parent = created_bones[\"shoulder_R\"]\n",
        "created_bones[\"forearm_R\"].parent = created_bones[\"upper_arm_R\"]\n",
        "created_bones[\"hand_R\"].parent = created_bones[\"forearm_R\"]\n",
        "\n",
        "# Pierna izquierda\n",
        "created_bones[\"hip_L\"].parent = created_bones[\"spine_lower\"]\n",
        "created_bones[\"thigh_L\"].parent = created_bones[\"hip_L\"]\n",
        "created_bones[\"shin_L\"].parent = created_bones[\"thigh_L\"]\n",
        "created_bones[\"foot_L\"].parent = created_bones[\"shin_L\"]\n",
        "\n",
        "# Pierna derecha\n",
        "created_bones[\"hip_R\"].parent = created_bones[\"spine_lower\"]\n",
        "created_bones[\"thigh_R\"].parent = created_bones[\"hip_R\"]\n",
        "created_bones[\"shin_R\"].parent = created_bones[\"thigh_R\"]\n",
        "created_bones[\"foot_R\"].parent = created_bones[\"shin_R\"]\n",
        "\n",
        "bpy.ops.object.mode_set(mode='OBJECT')\n",
        "print(\"‚úÖ Rig creado con torso dividido: cuello ‚Üí centro ‚Üí cadera ‚úÖ\")"
      ],
      "metadata": {
        "id": "QtoIdiPRZdx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicaci√≥n del flujo general\n",
        "Importaci√≥n del JSON y c√°lculo de coordenadas Se cargan los puntos clave detectados por MediaPipe, incluyendo articulaciones reales y puntos virtuales estrat√©gicos (como el centro del torso).\n",
        "\n",
        "Definici√≥n estructural del cuerpo humano Las conexiones √≥seas se definen en la lista bone_connections, especificando c√≥mo cada parte se conecta (por ejemplo: \"upper_arm_L\" va de 11 a 13).\n",
        "\n",
        "Creaci√≥n y jerarquizaci√≥n del rig En modo EDIT, se crean los huesos con edit_bones.new() y se vinculan unos a otros para reflejar la anatom√≠a humana. Finalmente, se sale a modo OBJECT para aplicar los cambios.\n",
        "\n",
        "Aplicaci√≥n de pose entre rigs (opcional, incluido al final del script) Un segundo bloque de c√≥digo compara las direcciones de huesos entre el StickmanRig y el rig objetivo (StickmanRig22) y transfiere la orientaci√≥n de manera autom√°tica utilizando quaternions."
      ],
      "metadata": {
        "id": "mnKNSISQaj7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö†Ô∏è Dificultades encontradas y aprendizajes\n",
        "A pesar de lograr construir un esqueleto b√°sico (stickman) con los puntos detectados por la inteligencia artificial y generar un rig dentro de Blender, no se logro que ese esqueleto se conecte correctamente al modelo 3D del personaje.\n",
        "\n",
        "Durante el proceso se intentaron varias cosas:\n",
        "\n",
        "Crear un esqueleto autom√°tico basado en los datos del archivo .json.\n",
        "\n",
        "Copiar la postura detectada desde el stickman al rig del personaje.\n",
        "\n",
        "Ajustar nombres, estructuras y posiciones para que coincidieran.\n",
        "\n",
        "Sin embargo, la conexi√≥n no funcion√≥ como se esperaba. El personaje no adopt√≥ la pose correctamente, o no reaccionaba bien al rig generado.\n",
        "\n",
        "Las posibles razones que se identificaron fueron:\n",
        "\n",
        "El esqueleto autom√°tico no coincid√≠a del todo con el del personaje.\n",
        "\n",
        "Blender requiere configuraciones adicionales para que un rig mueva una malla (como pesos de deformaci√≥n y enlaces internos).\n",
        "\n",
        "La integraci√≥n completa puede llevar m√°s tiempo y pruebas de las que tuvimos disponibles.\n",
        "\n",
        "Aun as√≠, el proyecto permiti√≥ comprobar que es posible generar una pose humana detectada por IA, representarla gr√°ficamente en 3D, y acercarse bastante al rigging autom√°tico. El resto del trabajo dej√≥ una buena base para seguir desarroll√°ndolo en el futuro."
      ],
      "metadata": {
        "id": "UA9bMjbJa_Be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusi√≥n\n",
        "\n",
        "Este proyecto demuestra la posibilidad de utilizar modelos de IA preentrenados para generar estructuras humanas interpretables y adaptarlas a entornos 3D como Blender. Aunque la integraci√≥n completa dentro de Blender no fue alcanzada, se valid√≥ un flujo funcional de ida y vuelta entre plataformas, y se consolid√≥ el uso de Python como puente entre visi√≥n por computadora e infograf√≠a.\n"
      ],
      "metadata": {
        "id": "ypyU7nU1JX8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursos\n",
        "\n",
        "- Google Colab\n",
        "- MediaPipe Pose\n",
        "- Blender 4.x\n",
        "- Python\n",
        "- JSON (exportaci√≥n de poses)\n"
      ],
      "metadata": {
        "id": "e1ex34bzJaMo"
      }
    }
  ]
}